\newpage
\begin{problem}{22}{5}{15}
  The seven scientists. $N$~datapoints $\{x_n\}$ are drawn from $N$~distributions, all of which are Gaussian with a common mean~$\mu$ but with different unknown standard deviations~$\sigma_{n}$.  What are the maximum likelihood parameter $\mu$,$\{\sigma_{n}\}$ given the data?  For example, seven scientists (A, B, C, D, E, F, G) with wildly-differing experimental skills to measure~$\mu$.  You expect some of them to do accurate work (i.e.,~to have small~$\sigma_n$), and some of them to turn in wildly inaccurate results (i.e.,~to have enormous~$\sigma_n$).  Table~\ref{tab:problem22.5.15} shows their seven results.  What is the~$\mu$, and how reliable is each scientist?

  \begin{table}[h]
    \centering
    \begin{tabular}{c|S}
      \hline
      Scientist & $x_n$     \\\hline
      A         & -27.020   \\\hline
      B         & 3.570     \\\hline
      C         & 8.191     \\\hline
      D         & 9.898     \\\hline
      E         & 9.603     \\\hline
      F         & 9.945     \\\hline
      G         & 10.056    \\\hline
    \end{tabular}
    \caption{Seven measurements $\{x_n\}$ of a parameter $\mu$ \\ by seven scientists each having his own noise-level $\sigma_n$.}\label{tab:problem22.5.15}
  \end{table}

  I hope that you agree that, intuitively, it looks pretty certain that A and B are both inept measurers, that D-G are better, and that the true value $\mu$ is somewhere close to~10.  But what does maximizing the likelihood tell you?
\end{problem}

Given~$\mu$, the probability that observer~$i$ with measurement standard deviation~$\sigma_i$ measures any single value,~$x_i$, is:

\[ \Pr\left(x_i \vert \mu,\sigma_i \right) = \frac{1}{\sqrt{2\pi\sigma_i^2}} \exp\left(\frac{-(x_i - \mu)^2}{2\sigma_i^2}\right) \text{.}\]

If there are $n$ observers, then the probability,~$p$, of the measurements~$\{x_i\}$ where ${i\in \{1,\ldots,n\}}$ is:

\[ p = \Pr\left({\{x_i\}} \vert \mu,\{\sigma_i\} \right) = \prod_{i=1}^n\frac{1}{\sqrt{2\pi\sigma_i^2}} \exp\left(\frac{-(x_i - \mu)^2}{2\sigma_i^2}\right) \text{.}\]

Taking the natural logarithm of this function yields:

\[ \ln p =  \sum_{i=1}^{n} \left(-\frac{1}{2}\ln\left(2\pi\sigma_i^2\right) - \frac{(x_i - \mu)^2}{2\sigma_i^2}\right) \text{.}\]

Since the natural logarithm function is strictly increasing, we can take the derivative of ${\ln p}$ with respect to $\mu$ to find the maximum likelihood for $\mu$.  This yields:

\begin{aligncustom}
  0   &= \sum_{i=1}^{N}{\frac{\left(x_i-\mu\right)}{\sigma_{i}^{2}}}\\
  \mu &= \frac{1}{\sum_{i=1}^{N}{\sigma_{i}^{-2}}} \sum_{i=1}^{N}{\frac{x_i}{\sigma_i^2}} \text{.}
\end{aligncustom}

For each $\sigma_{i}$, the maximum likelihood estimate is found by taking the partial derivative with respect to $\sigma_i$ yielding:

\begin{aligncustom}
  0   &= -\frac{2\pi\sigma_{i}}{4\pi\sigma_{i}^2} + \frac{\left(x_{i}-\mu\right)^{2}}{\sigma_{i}^{3}} \\
  \sigma_i &= \sqrt{\left(x_i-\mu\right)^2} = \abs{x_i-\mu} \text{.}
\end{aligncustom}