\newpage
\begin{problem}{22}{5}{15}
  The seven scientists. $N$~datapoints $\{x_n\}$ are drawn from $N$~distributions, all of which are Gaussian with a common mean~$\mu$ but with different unknown standard deviations~$\sigma_{n}$.  What are the maximum likelihood parameter $\mu$,$\{\sigma_{n}\}$ given the data?  For example, seven scientists (A, B, C, D, E, F, G) with wildly-differing experimental skills to measure~$\mu$.  You expect some of them to do accurate work (i.e.,~to have small~$\sigma_n$), and some of them to turn in wildly inaccurate results (i.e.,~to have enormous~$\sigma_n$).  Table~\ref{tab:problem22.5.15} shows their seven results.  What is the~$\mu$, and how reliable is each scientist?

  \begin{table}[h]
    \centering
    \begin{tabular}{c|S}
      \hline
      Scientist & $x_n$     \\\hline
      A         & -27.020   \\\hline
      B         & 3.570     \\\hline
      C         & 8.191     \\\hline
      D         & 9.898     \\\hline
      E         & 9.603     \\\hline
      F         & 9.945     \\\hline
      G         & 10.056    \\\hline
    \end{tabular}
    \caption{Seven measurements $\{x_n\}$ of a parameter $\mu$ \\ by seven scientists each having his own noise-level $\sigma_n$.}\label{tab:problem22.5.15}
  \end{table}

  I hope that you agree that, intuitively, it looks pretty certain that A and B are both inept measurers, that D-G are better, and that the true value $\mu$ is somewhere close to~10.  But what does maximizing the likelihood tell you?
\end{problem}

Given~$\mu$, the probability that observer~$i$ with measurement standard deviation~$\sigma_{n}$ measures any single value,~$x_{n}$, is:

\[ \Pr\left(x_{n} \vert \mu,\sigma_{n} \right) = \frac{1}{\sqrt{2\pi\sigma_{n}^2}} \exp\left(\frac{-(x_{n} - \mu)^2}{2\sigma_{n}^2}\right) \text{.}\]

If there are $N$~(independent) observers, then the likelihood,~$L$, of the measurements~$\{x_{n}\}$ where ${i\in \{1,\ldots,n\}}$ is:

\begin{aligncustom}
  L &= \Pr\left({\{x_{n}\}} \vert \mu,\{\sigma_{n}\} \right)\\
    &= \prod_{n=1}^{N} \Pr\left(x_{n} \vert \mu,\sigma_{n} \right)\\
    &= \prod_{n=1}^{N}\frac{1}{\sqrt{2\pi\sigma_{n}^2}} \exp\left(\frac{-(x_{n} - \mu)^2}{2\sigma_{n}^2}\right) \text{.}
\end{aligncustom}

Taking the natural logarithm of this function yields:

\[ \ln L =  \sum_{i=1}^{n} \left(-\frac{1}{2}\ln\left(2\pi\sigma_{n}^2\right) - \frac{(x_{n} - \mu)^2}{2\sigma_{n}^2}\right) \text{.}\]

Since the natural logarithm function is strictly increasing, we can take the derivative of ${\ln L}$ with respect to $\mu$ to find the maximum likelihood for $\mu$.  This yields:

\begin{aligncustom}
  0   &= \sum_{i=1}^{N}{\frac{\left(x_{n}-\mu\right)}{\sigma_{n}^{2}}}\\
  \mu &= \frac{1}{\sum_{i=1}^{N}{\sigma_{n}^{-2}}} \sum_{i=1}^{N}{\frac{x_{n}}{\sigma_{n}^2}} \text{.}
\end{aligncustom}

For each $\sigma_{n}$, the maximum likelihood estimate is found by taking the partial derivative with respect to $\sigma_{n}$ yielding:

\begin{aligncustom}
  0   &= -\frac{2\pi\sigma_{n}}{4\pi\sigma_{n}^2} + \frac{\left(x_{i}-\mu\right)^{2}}{\sigma_{n}^{3}} \\
  \sigma_{n} &= \sqrt{\left(x_{n}-\mu\right)^2} = \abs{x_{n}-\mu} \text{.}
\end{aligncustom}

To find the maximum likelihood,~$L_{\max}$, substitute the definition found above for~$\sigma_{n}$.  This yields

\begin{aligncustom}
  L_{\max} &= \prod_{i=1}^{n}\frac{1}{\sqrt{2\pi(x_{n}-\mu)^2}} \exp\left(\frac{-(x_{n} - \mu)^2}{2(x_{n}-\mu)^2}\right) \\ 
           &= \prod_{i=1}^{n}\frac{1}{\sqrt{2\pi(x_{n}-\mu)^2}} \exp\left(-\frac{1}{2}\right)\text{.}
\end{aligncustom}

Therefore, the likelihood is maximized when $\mu=x_{n}$.  For this problem, $x_{n}$~is any value in Table~\ref{tab:problem22.5.15} meaning the maximum likelihood is when any of the seven scientists are perfectly correct, i.e.,~always report the true~$\mu$ with no variation.  This result is disquieting as it means the likelihood is maximized if~$\mu$ is~-27.020 or~3.570.  As noted in the question itself, the visceral feeling is that the true~$\mu$ is around ten give or take.  Therefore, maximizing the likelihood may not always yield the best result.
