\documentclass{report}

\usepackage{fullpage}
\usepackage[skip=4pt]{caption} % ``skip'' sets the spacing between the figure and the caption.
\usepackage{tikz}
\usepackage{pgfplots}   % Needed for plotting
\pgfplotsset{compat=newest}
\usepgfplotslibrary{fillbetween}  % Allow for highlighting under a curve
\usepackage{amsmath}    % Allows for piecewise functions using the ``cases'' construct
%\usepackage{mathrsfs}   % Use the ``\mathscr'' command in an equation.
\usepackage{siunitx}    % Allows for ``S'' alignment in table to align by decimal point
\usepackage{enumitem}   % Reference enumerated item lists.

\usepackage[obeyspaces,spaces]{url} % Used for typesetting with the ``path'' command
\usepackage[hidelinks]{hyperref}   % Make the cross references clickable hyperlinks
\usepackage[bottom]{footmisc} % Prevents the table going below the footnote
\usepackage{nccmath}    % Needed in the workaround for the ``aligncustom'' environment
\usepackage{amssymb}    % Used for black QED symbol
\usepackage{bm}    % Allows for bolding math symbols.
\usepackage{tabto}     % Allows to tab to certain point on a line
\usepackage{float}

\newcommand{\hangindentdistance}{1cm}
\setlength{\parindent}{0pt}
\setlength{\leftskip}{\hangindentdistance}
\setlength{\hangafter}{1}
\setlength{\parskip}{1em}


% Set up page formatting
\usepackage{todonotes}
\usepackage{fancyhdr} % Used for every page footer and title.
\pagestyle{fancy}
\fancyhf{} % Clears both the header and footer
\renewcommand{\headrulewidth}{0pt} % Eliminates line at the top of the page.
\fancyfoot[LO]{CMPS218 \textendash\ Homework \#1} % Left
\fancyfoot[CO]{\thepage} % Center
\fancyfoot[RO]{Zayd Hammoudeh} %Right

% Change interline spacing.
\renewcommand{\baselinestretch}{1.1}
\newenvironment{aligncustom}
{ \csname align*\endcsname % Need to do this instead of \begin{align*} because of LaTeX bug.
    \centering
}
{
  \csname endalign*\endcsname
}
%--------------------------------------------------


\title{\textbf{CMPS218 \textendash\ Homework \#1}}
\author{Zayd Hammoudeh}

%---------------------------------------------------%
% Define the Environments for the Problem Inclusion %
%---------------------------------------------------%
\newcounter{subProbCount}       % Initialize the subproblem counter
\newcounter{problemCount}
\setcounter{problemCount}{0} % Reset the subproblem counter
\newenvironment{problemshell}{
  \par%
  \medskip
  \leftskip=0pt\rightskip=0pt%
}
{
  \par\medskip
  \setcounter{subProbCount}{1} % Reset the subproblem counter
}
\newenvironment{problem}[3]
{%
  \begin{problemshell}
    \noindent \textit{Chapter \##1.#2, Problem \##3} \\
    \bfseries
}
{
  \end{problemshell}
}
\newenvironment{subproblem}
{%
  \par%
  \medskip
  \leftskip=0pt\rightskip=0pt%
  \bfseries
  % Print the subproblem count and offset to the left
  (\alph{subProbCount}) \hangindent=\hangindentdistance \hangafter=1 \tabto{\hangindentdistance}
}
{
  \stepcounter{subProbCount} % Increment the subproblem counter
  \par\medskip
}
\newcommand{\problemspace}{\\[0.4em]}

\begin{document}
  \maketitle

  \begin{problem}{2}{9}{33}
    Two points are selected at random on a straight line segment of length~1.  What is the probability that a triangle can be constructed out of the three resulting segments?
  \end{problem}

  For the two randomly selected points,~$x_1$ and $x_2$, define $L_{\max}$ as the length of the longest segment.  By the triangle inequality theorem, it is clear that a triangle can only be formed from these three resulting segments if ${L_{\max} < 0.5}$.


  Consider the tuple ${(x_1,x_2)}$. Each point in the domain $(0,1)^2$ is equally likely since the points are selected randomly.  Figure~\ref{fig:problem2.9.33} shows the portions of this domain (highlighted in light blue) where the lines segments induced by $x_1$ and $x_2$ can be used to form a triangle.

  \begin{figure}[h]
    \centering
    \scalebox{0.6}{ % Scaling factor for the plot.
      \begin{tikzpicture}
      \begin{axis}[
        axis line style = thick,
        axis lines=middle,
        xmin=0,
        xmax=1,
        ymin=0,
        ymax=1,
        xlabel=$x_1$,
        ylabel={$x_2$},
      ]
      \addplot[name path=half, smooth, color=blue, domain=0:1] {0.5};
      \addplot[name path=f_1, smooth, color=blue, domain=0:0.5] {0.5 + x};
      \addplot[name path=f_2, smooth, color=blue, domain=0.5:1] {-.5+x};
      \draw [color=blue, thick] (0.5,0) -- (0.5,1);
      \path[name path=axis] (axis cs:0,0) -- (axis cs:1,0); % X axis for fill between
      \addplot [
        thick,
        color=blue,
        fill=blue,
        fill opacity=0.05
        ]
        fill between[
        of=f_1 and half,
        soft clip={domain=0:.5},
      ];
      \addplot [
        thick,
        color=blue,
        fill=blue,
        fill opacity=0.05
        ]
        fill between[
        of=f_2 and half,
        soft clip={domain=0.5:1},
      ];
      \end{axis}
      \end{tikzpicture}
    }
    \caption{Regions of the domain where the line segments induced by $(x_1,x_2)$ form a triangle.}\label{fig:problem2.9.33}
  \end{figure}

  Since the probability of selecting any point from the above domain is uniform, the probability of selecting points that form a triangle is found via:

  \begin{aligncustom}
    \Pr(\text{Form a triangle}) &= \frac{\text{Shaded Area}}{\text{Total Area}}\\
                                &= \frac{2 \cdot \left(\frac{1}{2}\right)^{3}}{1}\\
                                &= \boxed{\frac{1}{4}}
  \end{aligncustom}

  \newpage
  \begin{problem}{5}{8}{30}
    \textit{Scientific American} carried the following puzzle in 1975.
    \problemspace
    \textbf{The poisoned glass:} \textnormal{\textit{'Mathematicians are curious birds,' the police commissioner said to his wife. 'You see, we had all those partly filled glasses lined up in rows on a table in the hotel kitchen. Only one contained poison, and we wanted to know which one before searching the glass for fingerprints.  Our lab could test the liquid in each glass, but the tests take time and money, so we wanted to make as few of them as possible by simultaneously testing mixtures of small samples from groups of glasses.  The university sent over a mathematics professor to help us.  He counted the glasses, smiled and said: \\ ``Pick any glass you want, Commissioner. We'll test it first.'' \\ ``But won't that waste a test?'' I asked. \\ ``No,'' he said. ``it's part of the best procedure.  We can test one glass first.  It doesn't matter which one.'' \\ `How many glasses were there to start with?' the commissioner's wife asked. \\ 'I don't remember. Somewhere between 100 and 200.'} \\What was the exact number of glasses?}
    \problemspace
    Solve this puzzle and then explain why the professor is in fact wrong and the commissioner was right.  What is in fact the optimal procedure for identifying the one poisoned glass?  What is the expected waste relative to this optimum if one followed the professor's strategy?  Explain the relationship to symbol coding.
  \end{problem}

  The test for poison has a binary outcome, i.e.,~the sample either has poison or not.  Therefore, assuming each cup has poison with equal probability, the size of the remaining set of glasses is, on average, cut in half with each test.

  If the number of glasses,~$n$, is a power of~$2$, then the number of tests required is $\lg n$, where $\lg$ is the base-$2$ logarithm.  Note that the only power of~2 between 100 and 200 is 128.  There was one extra glass that the professor tested separately.  Therefore, there was \boxed{129~\text{glasses}}.

  \begin{table}[h]
    \centering
    \begin{tabular}{c|c|c}
      \hline
      Glass ID & Probability of Poison & \# Tests  \\\hline
      1        & 1/129                 & 1         \\\hline
      2-129    & 128/129               & 1 + 7 = 8 \\\hline
    \end{tabular}
    \caption{Number of tests required using the professor's strategy}\label{tab:problem5.9.20-Prof}
  \end{table}

  Table~\ref{tab:problem5.9.20-Prof} shows the number of tests required when using the professor's strategy.  Glass~\#1 represents the first glass tested, i.e.,~the one selected at random.  In the unlikely event that glass has the poison, no additional testing is required. In contrast, if the poison is in one of the other 128~glasses, seven tests (plus the additional one for the first glass) are required.  Using this strategy, the expected number of tests is:

  \begin{aligncustom}
    \mathbb{E}(\text{Professor's Strategy}) &= \frac{1}{129} \cdot 1 + \frac{128}{129} \cdot 8 \\
                                            &\approx \boxed{7.946}\text{.}
  \end{aligncustom}


  In contrast, the optimal strategy is:

  \begin{enumerate}
    \item Select one glass at random and leave it off to the side.
    \item\label{itm:test} Test a sample that combines wine from half of the remaining glasses (excluding the one off to the side).
    \item\label{itm:discard} If poison is observed in this tested sample, discard the untested glasses.  Otherwise, discard the tested glasses.
    \item Repeat steps~\#\ref{itm:test} and~\#\ref{itm:discard} until only a single glass remains (excluding the one off to the side).
    \item If poison was ever observed in any of the previous tests, then the remaining glass not off to the side has the poison, and no additional testing is required.
    \item\label{itm:testRemaining} If poison was never observed in any test, then test just the remaining glass not off to the side. If poison is detected, then the answer is clear, and the tested glass has poison; otherwise, the glass off to the side has the poison.
  \end{enumerate}

  Table~\ref{tab:problem5.9.20-Opt} shows the number of tests required using this optimum strategy.    Note that Glass\#~129 entails the remaining glass in step~\#\ref{itm:testRemaining} where no sample tests positive for poison up to the last remaining glass not off to the side.

  \begin{table}[h]
    \centering
    \begin{tabular}{c|c|c}
      \hline
      Glass ID & Probability of Poison & \# Tests  \\\hline
      1        & 1/129                 & 7 + 1     \\\hline
      2-128    & 127/129               & 7         \\\hline
      129      & 1/129                 & 7 + 1     \\\hline
    \end{tabular}
    \caption{Number of tests required using the optimum strategy}\label{tab:problem5.9.20-Opt}
  \end{table}

  \noindent
  Using this optimum strategy, the expected number of tests is:

  \begin{aligncustom}
    \mathbb{E}(\text{Optimum Strategy}) &= \frac{2}{129} \cdot 8 + \frac{127}{129} \cdot 7 \\
    &\approx \boxed{7.016}\text{.}
  \end{aligncustom}

  It is clear then that the expected waste of the professor's strategy is \boxed{0.93} tests.

  Maximum compression of a symbol code is achieved by assigning shorter codes (i.e.,~with less bits) to outcomes with higher probability.  In contrast, the professor prioritized the least likely outcome by testing the randomly selected glass first.  The optimal strategy described above always tests the most likely outcome (i.e.,~more glasses at once) similar to how symbol codes are encoded.


  \newpage
  \begin{problem}{20}{2}{2}
    Show that as the stiffness~$\beta$ goes to~$\infty$, the soft K-means algorithm becomes identical to the original hard K-means algorithm except for the way in which means assigned no points behave.  Describe what those means do instead of sitting still.
  \end{problem}

  In the standard or ``hard'' K-means algorithm, each point is assigned to exactly one cluster.  As such, each of a cluster's points have equal membership.

  Soft K-means reduces the rigidity of the standard K-means by introducing a new ``stiffness'' hyperparameter,~$\beta$.  Rather than each point being a member of exclusively one cluster, the \textit{responsibility} for that point is shared (generally unevenly) among all $K$~clusters.  For cluster~$k$ and point~$\textbf{x}^{(n)}$, the responsibility,~$r_{k}^{(n)}$ is:

  \begin{equation}
    r_k^{(n)} = \frac{\exp(-\beta d(\textbf{m}^{k'}, \textbf{x}^{(n)}))}{\sum_{k} \exp(-\beta d(\textbf{m}^{k'}, \textbf{x}^{(n)}))}
  \end{equation}

  \noindent
  where $d$ is the distance metric, and $m^{k}$ is the center of cluster~$k$.

  As $\beta$ increases, then even small differences in $d$ can cause massive changes in responsibility.  As ${\beta \rightarrow \infty}$, all responsibility for a point will be assigned to its nearest cluster.  This behavior is exactly the same as standard, ``hard'' K-means where points belong to only the cluster's whose centroid is closest.

  When performing Soft K-means with ${\beta \rightarrow \infty}$, it is possible that some clusters may have no responsibility for no points.  In which case, the centroid approaches the zero vector,~$\vec{0}$.

  \newpage
  \begin{problem}{22}{5}{15}
    The seven scientists. $N$~datapoints $\{x_n\}$ are drawn from $N$~distributions, all of which are Gaussian with a common mean~$\mu$ but with different unknown standard deviations~$\sigma_{n}$.  What are the maximum likelihood parameter $\mu$,$\{\sigma_{n}\}$ given the data?  For example, seven scientists (A, B, C, D, E, F, G) with wildly-differing experimental skills to measure~$\mu$.  You expect some of them to do accurate work (i.e.,~to have small~$\sigma_n$), and some of them to turn in wildly inaccurate results (i.e.,~to have enormous~$\sigma_n$).  Table~\ref{tab:problem22.5.15} shows their seven results.  What is the~$\mu$, and how reliable is each scientist?

    \begin{table}[h]
      \centering
      \begin{tabular}{c|S}
        \hline
        Scientist & $x_n$     \\\hline
        A         & -27.020   \\\hline
        B         & 3.570     \\\hline
        C         & 8.191     \\\hline
        D         & 9.898     \\\hline
        E         & 9.603     \\\hline
        F         & 9.945     \\\hline
        G         & 10.056    \\\hline
      \end{tabular}
      \caption{Seven measurements $\{x_n\}$ of a parameter $\mu$ by seven scientists each having his own noise-level $\sigma_n$.}\label{tab:problem22.5.15}
    \end{table}

    I hope that you agree that, intuitively, it looks pretty certain that A and B are both inept measurers, that D-G are better, and that the true value $\mu$ is somewhere close to~10.  But what does maximizing the likelihood tell you?
  \end{problem}

  Given $n$ observers that each make a single observeration with a common mean $\mu$ and standard deviations $\{\sigma_n\}$, the maximum likelihood mean, $\bar{x}$, is:

  \[\bar{x} = \frac{\sum_{i=1}^{n}x_i}{n}\textrm{.}\]

  \noindent
  In the case of the seven scientists, the sample mean is:

  \begin{aligncustom}
    \bar{x} &= \frac{-27.020 + 3.570 + 8.191 + 9.898 + 9.603 + 9.945 + 10.056}{7}\\
            &= \boxed{3.463}
  \end{aligncustom}

  For observer $i$ where ${1 \leq i \leq n}$, the maximum likelihood standard deviation is:

  \[ \sigma_i = \lvert x_i - \mu \rvert \]

  \noindent
  since there is only one sample per distribution. Table~\ref{tab:problem22.5.15Stdev} lists the maximum likelihood standard deviations for the seven scientists.

  \begin{table}[h]
    \centering
    \begin{tabular}{c|S}
      \hline
      Scientist & $x_n$     \\\hline
      A         & 30.483    \\\hline
      B         & 0.107     \\\hline
      C         & 4.728     \\\hline
      D         & 6.435     \\\hline
      E         & 6.140     \\\hline
      F         & 6.482     \\\hline
      G         & 6.593    \\\hline
    \end{tabular}
    \caption{Maximum likelihood $\sigma$ for the seven scientists}\label{tab:problem22.5.15Stdev}
  \end{table}

  From the data, it appears that scientists D\textendash G are reliable.  Scientist C appears less reliable than them, but better than A and B which appear to be the worst.

  Clearly for this problem, maximizing the likelihood does not yield the most plausible outcome.  There are four observers that essentially measure the same value; it is theoretically possible these measurements are due to coincidence.  However, that it is unlikely given the problem description.  Therefore, relying blindly on the maximum likelihood calculation may lead to poor conclusions.

\end{document}

