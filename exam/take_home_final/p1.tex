\begin{problem}{4}{14}
  You are given 12 balls and the three-outcome balance of exercise 4.1; this time, two of the balls are odd; each odd ball may be heavy or light, and we don't know which.  We want to identify the odd balls and in which direction they are odd.
\end{problem}

  The hypothesis class~$\mathcal{H}$ is the set of all valid allocations of odd ball(s) in the problem. The number of weights required to determine the correct hypothesis,~$h\in\mathcal{H}$ can be estimated using the equation:

  \begin{equation}\label{eq:p1NumbWeightsRequired}
    \#\text{Weights Required} \approx \ceil*{\frac{\log_{2} \abs{\mathcal{H}}}{H}} \text{,}
  \end{equation}

  \noindent
  where $H$ is the balance outcomes' binary entropy.  The logic for this formula is quite intuitive. Representing each element in~$\mathcal{H}$ requires ${\log_{2}\mathcal{H}}$~bits.  Binary entropy is the \textit{expected} information gain per weighing (in bits).  Dividing the two yields the \textit{expected number of weights}.

  When there is a single odd ball out of twelve, the cardinality of the hypothesis class,~$\abs{\mathcal{H}}$, is ${2 * \binom{12}{1} = 24}$. Each outcome of the balance partitions~$\mathcal{H}$ into subsets of roughly equal probability. This is illustrated in Table~\ref{tab:p1OneBallTable} which shows the correspondence between odd ball balance location and the outcome. Using Equation~\eref{eq:p1NumbWeightsRequired} for the single odd ball, we estimate number of weights required is approximately

  \[ \ceil*{\frac{\log_{2}24}{1.585}} \approx \ceil*{\frac{4.584}{1.585}} \approx \ceil{2.893} = 3 \text{.} \]

  \noindent
  This estimate equals the actual number of weights as shown in Figure~4.2 of Mackay's text.

  \begin{table}[H]
    \centering
    \caption{Three-outcome balance position for a \\single odd ball that is light or heavy}\label{tab:p1OneBallTable}
    \begin{tabular}{|c||c|c||c|}
      \hline
      \multirow{2}{*}{Case~\#} &  \multicolumn{2}{c||}{Odd Ball} & Balance\\\cline{2-3}
      &  Weight  &  Location &  Position  \\ \hline\hline
      1   &  Light   &  Left     &  $\bar{~}\bar{~}|\underline{~~}$ \\ \hline
      2   &  Heavy   &  Right    &  $\bar{~}\bar{~}|\underline{~~}$ \\ \hline\hline
      3   &  Light   &  Right    &  $\underline{~~}|\bar{~}\bar{~}$ \\ \hline
      4   &  Heavy   &  Left     &  $\underline{~~}|\bar{~}\bar{~}$ \\ \hline\hline
      5   &  Light   &  Neither  &  $-|-$ \\ \hline
      6   &  Heavy   &  Neither  &  $-|-$ \\ \hline
    \end{tabular}
  \end{table}


\begin{subproblem}
  \textit{Estimate} how many weights are required by the optimal strategy.  And what if there are three odd balls?
\end{subproblem}

  When there are two odd balls, $\mathcal{H}$ grows by more than an order of magnitude from~$24$ to ${2\cdot2\cdot\binom{12}{2} = 264}$. In addition, interpreting the balance outcomes is more difficult as shown in Table~\ref{tab:p1TwoBallsUnknownWeights}.  Note first that the number of possible cases has increased six-fold from~6 in Table~\ref{tab:p1OneBallTable} to~36.  Furthermore, we are given no knowledge about the relative weights of a normal, light or heavy ball.  This prevents us from drawing conclusions about the balance's behavior when there are mismatched balls on the same side of the balance.

  \input{p1_two_ball_table}

  Observe that Table~\ref{tab:p1TwoBallsUnknownWeights} has a column labeled ``Probability''; it denotes the likelihood of a balance configuration.  $P_{1}$~and $P_{2}$ in that table equal~$\frac{3}{154}$ and~$\frac{4}{121}$, respectively.  Table~\ref{tab:twoBallProbabilityBreakdown} lists the grouped probability of each balance outcome.\footnote{The calculations assumes an equivalent number of balls in the left and right balance as well as off the scale (similar to Mackay's approach).}  Using this table to calculate binary entropy is problematic as the ``Unknown'' cases do not fit neatly into any three of the balance outcomes.  For simplicity, we divide the probability for the ``Unknown'' cases equally across the three outcomes.  Therefore, this \textit{approximated entropy} equals equals~$1.552$.   Using Eq.~\eref{eq:p1NumbWeightsRequired}, we estimate the number of weights required for two odd balls is:

  \[ \ceil*{\frac{\log_{2} 264}{H_{approx}}} \approx \ceil*{\frac{8.044}{1.552}} \approx \ceil*{5.182} = \boxed{6} \text{.} \]

  When there are three odd balls, $\abs{\mathcal{H}}$ equals~${2\cdot2\cdot2\cdot\binom{12}{3} = 1,760}$.  Estimating the entropy is even more challenging as the probability of an ``Unknown'' configuration is higher.  To simplify the calculation, we reuse the approximated entropy from two ball case.  This results in an estimate of

  \[ \ceil*{\frac{\log_{2} 1760}{H_{mod}}} \approx \ceil*{\frac{10.78}{1.552}} \approx \ceil*{6.945} = \boxed{7}\]

  \noindent
  for the expected number of weights.

  \begin{table}
    \centering
    \caption{Probability partition by balance outcome for two odd balls of unknown relative weights}\label{tab:twoBallProbabilityBreakdown}
    \begin{tabular}{|c||c|c|c|c|}
      \hline
      Balance Position & $\bar{~}\bar{~}|\underline{~~}$  & $\underline{~~}|\bar{~}\bar{~}$  & $-|-$ & Unknown \\\hline
      Probability      & $\frac{603}{1694} \approx 0.356$ & $\frac{603}{1694} \approx 0.356$ & $\frac{178}{847} \approx 0.210$ & $\frac{12}{154} \approx 0.078$    \\\hline
    \end{tabular}
  \end{table}

\begin{subproblem}
  How do your answers change if it is known that all the regular balls weigh 100g, that light balls weight 99g, and heavy ones weigh 110g?
\end{subproblem}

  In part~(a), uncertainty about the balance's behavior in some cases necessitated that we approximate the binary entropy.  Now that the relative ball weights are known, Cases~\#33-34 and Cases~\#35-36 in Table~\ref{tab:p1TwoBallsUnknownWeights} are assigned to ``${\underline{~~}|\bar{~}\bar{~}}$'' and ``${\bar{~}\bar{~}|\underline{~~}}$'' respectively.  The updated probability breakdown for each balance outcome is shown in Table~\ref{tab:twoBallProbabilityBreakdownKnownWeights}.  This corresponds to a binary entropy of~$1.531$.  Note that even though we have more information, the binary entropy went down.  That is because when approximating the entropy, we spread the probability corresponding to the ``Unknown'' case evenly across the three balance outcomes.  This lead to a more balanced probability mass function (PMF), which in turn led to a higher entropy.

  \begin{table}
    \centering
    \caption{Updated probability partition with known ball weights in part (b)}\label{tab:twoBallProbabilityBreakdownKnownWeights}
    \begin{tabular}{|c||c|c|c|}
      \hline
      Balance Position & $\bar{~}\bar{~}|\underline{~~}$  & $\underline{~~}|\bar{~}\bar{~}$  & $-|-$ \\\hline
      Probability      & $\frac{669}{1694} \approx 0.395$ & $\frac{669}{1694} \approx 0.395$ & $\frac{178}{847} \approx 0.210$   \\\hline
    \end{tabular}
  \end{table}

  Using the updated entropy, the expected number of weights for two balls remains~6.  For three balls, the estimated number of weights increased to~${\ceil*{7.042} = 8}$.  However, it would not be correct to assign this large change in the number of weights entirely to the updated entropy.  Instead, a relatively small change in the expected count was magnified by the ceiling function in Equation~\eref{eq:p1NumbWeightsRequired}.

